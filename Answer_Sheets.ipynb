{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HARINI-RadheyKrishna/Machine_Learning/blob/master/Answer_Sheets.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EE 599 Reading Assignment \\# 1:\n",
        "You can access paper using this [link](https://courses.cs.washington.edu/courses/cse550/21au/papers/CSE550.Eyeriss.pdf), do not re-distribute the paper."
      ],
      "metadata": {
        "id": "JA0rETRQZBON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q1: (1 Point)\n",
        "\n",
        "**Disallowed** list:\n",
        "- You **MAY NOT** collaborate with anyone else on this assignment. This means you cannot talk to anyone else about the assignment until after deadline.\n",
        "- You **MAY NOT** use ChatGPT and services like that\n",
        "\n",
        "**Allowed** list:\n",
        "- Notes including any slides from the class\n",
        "- The textbooks\n",
        "- The given paper"
      ],
      "metadata": {
        "id": "s-HWWv6kaxrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A1:\n",
        "\n",
        "I affirm I have read these exam rules and will follow them. Failure to do so may subject me to sanctions including an F in the course.\n",
        "\n",
        "**Type your full name to affirm you have read the above statement:**\n"
      ],
      "metadata": {
        "id": "Pz0uD-nHbHuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Harini Thirunavukkarasu"
      ],
      "metadata": {
        "id": "wc-IS2a3zO1l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "## Q2.1 Summary (24 Points):\n",
        "\n",
        "Summarize the main objectives and contributions of the paper."
      ],
      "metadata": {
        "id": "XzNWLVCLbcjq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A2.1:\n",
        "\n"
      ],
      "metadata": {
        "id": "0j5A4_Zfbpra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Q2.2 Comprehension (15 Points):\n",
        "- What problem is the paper addressing?\n",
        "- How does the Eyeriss architecture differ from other architectures you are familiar with?"
      ],
      "metadata": {
        "id": "IE2n5UQHbuZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A2.2:\n",
        "\n",
        "**Problem addressed in the paper:**\n",
        "\n",
        "The problem discussed in the paper is regarding the performance evaluation of a Convolutional Neural Network by other authors by only considering the optimizations performed on the on-chip processor and not on the off-chip DRAM Chips.\n",
        "\n",
        "The authors of this paper putforths a valid argument and later proves that data accesses from memory largely contributes to energy usage and hence any optimization needs to be visualized using both on-chip and off-chip data structures to assess the energy efficiency of a system.\n",
        "\n",
        "**Differences explored in Eyeriss architecture:**\n",
        "\n",
        "  The key differences which I found to be prominent in this architecture are:\n",
        "  \n",
        "  i. The Eyeriss architecture optimizes the performance by taking into account both the on-chip and off-chip architectures of the CNN. It performs the optimization by implementing many techniques like  \n",
        "\n",
        "    --- Data reuse using weight stationary, input stationary and output\n",
        "        stationary techniques\n",
        "\n",
        "    --- Data compression to reduce computational power\n",
        "\n",
        "    --- Data gating to remove the calculations involving zeros and thereby\n",
        "        reducing power\n",
        "\n",
        "    --- Implementation of multiple levels of memory heirarchy to reduce\n",
        "        the data access delays\n",
        "\n",
        "\n",
        "  ii. Another difference I observed is the presence of the RLC Coder Decoder unit to save the memory bandwidth by compression of the bits for storing purposes.\n",
        "\n",
        "  The accessing of data from the DRAM (or off-chip memory unit) to the GLB (Global Memory Buffer) costs us huge bandwidth requirement if the data is stored in bits having sizes of 64 bits to 32 bits.\n",
        "  \n",
        "  Here they use an encoding scheme to convert the 64/32 bits into 16bits thereby **cutting down the bandwidth requirement to 4 or 2 times** the original requirement.\n",
        "\n",
        "  iii. The implementation of custom NoC with data gating capabilities to reduce the power demands which enables the data movement from those PEs whose address matches with the Network IDs while the rest of the PEs are gated and remain inactive.\n",
        "\n",
        "  The design of the NoC also includes the implementation of a configurable memory bank which supports the movement of the data with adequate bandwidth. This enables mapping the Processing Elements to the Processing Arrays without any restriction for placements but with an additional tag structure to locate the data associated with the PEs.\n",
        "\n",
        "  iv. Implementation of multicast controller to match the Tag-IDs of the data items in PEs with the network items. This also helps in masking those PEs to not access data from them and thereby put them into sleep mode."
      ],
      "metadata": {
        "id": "9O7k2DbXcNp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Q2.3 Technical Deep Dive (15 Points):\n",
        "- Describe the spatial architecture of Eyeriss in your own words.\n",
        "- How does the Eyeriss architecture optimize energy efficiency in dataflow for CNNs?\n",
        "- What are the main challenges in designing an energy-efficient architecture for CNNs, and how does Eyeriss tackle these challenges?"
      ],
      "metadata": {
        "id": "PnzcxvhWckY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A2.3:\n",
        "\n",
        "1. clock domains\n",
        "\n",
        "  fifos\n",
        "\n",
        "  decoding encoding units\n",
        "\n",
        "  spatial array\n",
        "\n",
        "  NoC\n",
        "\n",
        "  memory heirarchy\n",
        "\n",
        "  scan chain bits\n",
        "\n",
        "2. The Eyeriss architecture implements a Row Stationary method of dataflow management which enables it to maximize the data resuability thereby reducing the number of accesses to the DRAM or main memory from the local memory or on-chip memory.\n",
        "\n",
        "  Apart from the Row Stationary method, it also implements other schemes like   \n",
        "\n",
        "  --- restructuring the convolution dimensions to 1D and implementing them on the Processing Array by splitting the data elements equally across the 12 X 14 PEs.\n",
        "\n",
        "  --- It implements data compression techniques\n",
        "\n",
        "  --- Skipping of zeros to reduce computations\n",
        "\n",
        "  --- Implementing Memory Heirarchy\n",
        "\n",
        "  --- Implementation of custom NoC to reduce the data movement between inter PEs and also between PEs and GLBs.\n",
        "\n",
        "  --- Introducing encoding and decoding schemes to store data with a reduced storage space.\n",
        "\n",
        "\n",
        "3. Energy efficiency comes with a cost of hardware implementation overhead. The network should have enough bandwidth to support the huge movement of data. The distribution of data over the entire hardware using efficient software algorithms. The scalability of the design and configurability of the design."
      ],
      "metadata": {
        "id": "sv3rFNjpcuID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Q2.4 Evaluation (15 Points):\n",
        "- What were the key results or findings of the paper? Were they compelling?\n",
        "- How do the authors validate their claims or results? Are there any weaknesses in their methodology?"
      ],
      "metadata": {
        "id": "HRBvFutTc2uh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A2.4:\n",
        "\n",
        "1. The authors tried to prove that the overall energy efficiency in a system not just depends on optimizing the on-chip resources but also by optimizing the off-chip resources and they were able to design and fuctionally prove this theory.\n",
        "\n",
        "  The results published in the paper were able to evidently showcase the improvement of the Eyeriss processor in comparision to the other neural network resources. The authors tried to benchmark their model on 2 different CNNs and have achieved nearly around 230 to 280 mW where the typical power consumption values ranges from 200 to 400 mW.\n",
        "\n",
        "2. The authors have validated their claims by benchmarking their Eyeriss architecture on 2 CNN models namely AlexNet and VGG-16 and have also provided a comparative analysis on the peak performance obtained while they were running.\n",
        "\n",
        "  The typical values of power is around 200 - 400 mW and our Eyeriss is closely in 250 mW range on an average and hence can be considered as one of the most energy efficient schemes for deep neural networks.\n",
        "\n",
        "  The typical range values are obtained from the internet and I used it to compare the models. The results were compelling and can be implemented for all other CNN models too.\n",
        "\n",
        "  **Weakness for Eyeriss:**\n",
        "\n",
        "  The authors have not considered factors like\n",
        "\n",
        "  --- hiding data access time over data computation time\n",
        "\n",
        "  --- power gating\n",
        "\n",
        "  --- clock gating\n",
        "\n",
        "  --- low-power methodologies\n",
        "\n",
        "  --- hardware optimizations etc to further improve their architecture.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xKUL8nhTdNLZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Q2.5 Contextual Understanding (10 Points):\n",
        "- How does this work fit into the larger context of neural network architectures and energy efficiency?\n",
        "- Can the principles of Eyeriss be applied to other deep learning architectures beyond CNNs?\n"
      ],
      "metadata": {
        "id": "Fp_FV2bzdV4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A2.5:\n",
        "\n",
        "1. Eyeriss is a customized version of CNN which focuses on optimizing energy on convolutional models and are also proved to achieve reduced power consumption (with reference from the paper). So for a model which is desired to have efficient energy consumption schemes and also to perform deep convolutions on neural networks, this Eyeriss architecture can be considered as the benchmark for them. In future we can expect to see more generalized architectures with energy efficient schemes implemented on them by taking Eyeriss as a primitive model.\n",
        "\n",
        "  Techniques which are already in the industry are:\n",
        "\n",
        "  --- Data gating to improve efficiency\n",
        "\n",
        "  --- Compression techniques\n",
        "\n",
        "  --- Memory heirarchy to reduce data access delay\n",
        "\n",
        "  --- Exploiting data parallelism to the maximum and hence efficiently reusing the data using row stationary dataflow technique\n",
        "\n",
        "2. Of course yes. The energy efficient schemes can be implemented in other deep learning architectures but we have to keep in mind that the Eyeriss is a custom made design and hence if we are to implement it in other architectures, we need to restructure the hardware architecture to meet the needs of the other models.\n",
        "\n",
        "  All other concepts like data compression etc listed above are suitable to be implemented in other neural networks after designing the suitable hardware to support them."
      ],
      "metadata": {
        "id": "OvpxYhvNdg18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Q2.6 Discussion and Critique (10 points):\n",
        "- Are there any assumptions made by the authors that you disagree with or find questionable?\n",
        "- Do you think there are potential improvements or future directions not addressed by the authors?\n",
        "- How would you compare Eyeriss with other architectures or solutions you know of?"
      ],
      "metadata": {
        "id": "cxUpnGmtdjfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A2.6:\n",
        "\n",
        "1. i. The authors have talked about mapping 2D onto a 1D spad size. But he has not mentioned about how to process the mapping of multi-dimensional data onto a 1D spad size. For eg. As we introduce deep neural layers, there can be multiple dimensions in each layer and the reduction of those layers to 1D is inevitable but the author hasn't mentioned about this anywhere in the paper.\n",
        "\n",
        "  ii. The author have proposed about the gating aspects of the mathematical operations and has not mentioned about the hardware aspects of the processor and memory chip as concepts like power-gating, clock-gating are proven and implemented in modern day systems to reduce power consumption. I feel he is oriented only towards the data reusability to improve the energy efficiency. Power-gating of DRAM Modules are some aspects that can be implemented.\n",
        "\n",
        "  iii. The authors have mentioned about the reconfigurable GLB Memory structure but has failed to address the latency involved during the configuration and how much will it add to the entire delay percentage.\n",
        "\n",
        "  iv. From Fig 16 we can clear understand that the clock network is consuming nearly 40 to 60% of the total power requirements. But I wonder why the authors did not implement any synthesis methods or gating methods to reduce the clock network energy needs. Same goes with the scratch pads power requirements. Had power management schemes be implemented, efficiency would have increased multifold.\n",
        "\n",
        "  v. Finally, the authors have scaled the voltage below 1V but have not mentioned about the consequences of Leakage current for low supply voltages. Usually the relation between Vdd and leakage current is exponential and hence reducing the power supply even in magintude of 0.1V will drastically increase the leakage current by nearly 10 times. I suspect the increased power consumption by scratch pads and clock network could potentially due to the above reason as well.\n",
        "\n",
        "\n",
        "2. i. Implementation of schemes which can process and load data in a pipelined manner can be a good way to hide the access latency behind the processing latency.\n",
        "\n",
        "  ii. Further improvements can be contributed the implementation of power and clock gating concepts to maximize efficiency.\n",
        "\n",
        "  iii. Increasing the number of Procesing elements and thereby increasing the size of the Processing Arrays can help to improve the computational demands and performance ultimately.\n",
        "\n",
        "  iv. Certain other improvements in the field of circuit design, low-power design etc can improve the model further.\n",
        "\n",
        "  Overall, the collaboration between hardware and software teams can lead to a significant improvement in performance and energy efficient models in the future.\n",
        "\n",
        "\n",
        "3. i. Eyeriss is best suited for mobile and battery powered applications due to the very low power demands but other architectures of TPU and GPU are not suited for battery applications.\n",
        "\n",
        "  ii. Eyeriss is only intended for convolution and deep neural network based applications and not efficient for other models. Whereas other architectures are good for both graphics tasks and general purpose applications.\n",
        "\n",
        "  iii. As we know that Eyeriss is customised and not a generalized version, hence the cost of production is expected to be more than the general purpose architectures.\n",
        "  "
      ],
      "metadata": {
        "id": "cjAQy1qLdvZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Q2.7 Reflection (10 Points):\n",
        "- What was the most surprising or counterintuitive thing you learned from this paper?\n",
        "- How has reading this paper influenced your views on the importance of energy efficiency in deep learning?"
      ],
      "metadata": {
        "id": "hSMCnPg8d_0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A2.7:\n",
        "\n",
        "1.i. The most surprising thing I found was the area breakdown of the Eyeriss architecture.\n",
        "\n",
        "Usually those structures which are built using SRAMs or DRAMs occupy the most area on a chip.\n",
        "\n",
        "But to my surprise, it was the processing elements implemented using register files that occupied around 2/3 of the entire space.  \n",
        "\n",
        "ii. The architecture is configured to map the computational needs of a CNN to maximize reusability of data.\n",
        "\n",
        "In the world of hardware architecture, it is usually the softwares which will be modified to fit the hardware architecture as the hardware design changes are hard to re-configure.\n",
        "\n",
        "But here we implement a solution that configures the hardware architecture to efficiently process the convolution operation in a way that there is maximum utilization of data.\n",
        "\n",
        "2. My views:\n",
        "\n",
        "i. Efficiency in the context of energy does not only depend on restricting the number of hardware resources but also depends on the uutilization of those resources to improve the efficiency of a system. i.e. Proper utilization of the hardware resources by scheduling every Functional Unit with some data and keeping it occupied will improve efficiency. And this is important as energy is defined as energy * time which defines the maximum amount of work extracted from a system in a short amount of time will yield an optimal or reduced energy. This also means that running all the PEs at once will yield maximum energy efficiency as we are utilizing all the resources for a short amount of time.\n",
        "\n",
        "ii. Though memory access contribute a lot to the energy needs of a system, there are options to achieve maximum efficiency by reusing of data and by exploiting parallelism in data. These mean that utilizing the software techniques in a system contributes to the hardware too. i.e. We give emphasis to software techniques to utilize the hardware efficiently.  "
      ],
      "metadata": {
        "id": "nBtgEgPXeNR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Turn in your reading assignment by saving this answer sheet back to the Github repository."
      ],
      "metadata": {
        "id": "mnRyd9Iie6Dz"
      }
    }
  ]
}