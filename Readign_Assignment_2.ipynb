{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HARINI-RadheyKrishna/Machine_Learning/blob/master/Readign_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EE 599 Reading Assignment \\# 2:\n"
      ],
      "metadata": {
        "id": "JA0rETRQZBON"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q1: (1 Point)\n",
        "\n",
        "**Disallowed** list:\n",
        "- You **MAY NOT** collaborate with anyone else on this assignment. This means you cannot talk to anyone else about the assignment until after deadline.\n",
        "- You **MAY NOT** use ChatGPT and services like that\n",
        "\n",
        "**Allowed** list:\n",
        "- Notes including any slides from the class\n",
        "- The textbooks\n",
        "- The given paper"
      ],
      "metadata": {
        "id": "s-HWWv6kaxrx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A1:\n",
        "\n",
        "I affirm I have read these exam rules and will follow them. Failure to do so may subject me to sanctions including an F in the course.\n",
        "\n",
        "**Type your full name to affirm you have read the above statement:**\n"
      ],
      "metadata": {
        "id": "Pz0uD-nHbHuT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Harini Thirunavukkarasu"
      ],
      "metadata": {
        "id": "PldisnWYGFdW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q2 (99 Points):\n",
        "\n",
        "Use this [paper](https://web.stanford.edu/class/ee384m/Handouts/HowtoReadPaper.pdf) as your guideline on how to read papers in general.\n",
        "\n",
        "For this assignment, please write a summary of this paper:\n",
        "\n",
        "\"In-Datacenter Performance Analysis of a Tensor Processing Unit\"\n",
        "\n",
        "Complete info:\n",
        "\n",
        "Jouppi, Norman P., Cliff Young, Nishant Patil, David Patterson, Gaurav Agrawal, Raminder Bajwa, Sarah Bates et al. \"In-datacenter performance analysis of a tensor processing unit.\" In Proceedings of the 44th annual international symposium on computer architecture, pp. 1-12. 2017.\n",
        "\n",
        "You can download it from [here](https://arxiv.org/pdf/1704.04760.pdf?mod=article_inline)."
      ],
      "metadata": {
        "id": "IzxAsb2ErvaT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "## Q2.1 Summary (19 Points):\n",
        "\n",
        "Summarize the main objectives and contributions of the paper."
      ],
      "metadata": {
        "id": "XzNWLVCLbcjq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A2.1:\n",
        "\n",
        "**Objectives:**\n",
        "\n",
        "The  objectives of the paper are listed below:\n",
        "\n",
        "1. To evaluate the performance of TPU with other computer processors like GPU and CPU and comprehend how TPUs help in accelerating the performance during the inference phase.\n",
        "\n",
        "2. It also addresses the answer to the question - Why TPUs are so popular in datacenters.\n",
        "\n",
        "**Contributions:**\n",
        "\n",
        "**1. Introduction:** The paper describes about how the TPU architecture is excessively utilized for neural networks in datacenters. It dives a little into how neural networks are evolving, its types and how real-time data processing and result conclusion is important for datacenters at the same time maintaining accuracy of the output. Then it moves on to talk about how TPU serves all the above purposes. It describes the architecture of the TPU to describe how the real-time inference generation is achieved. Then they draft a comparision between TPU, GPU and CPU by deploying them with a Neural Network to mark the differences and compare them with a benchmark - called the Roofline model which gives the expected performance. They talk a little about energy requirements for all the processor architectures. They also talk about different TPU architectures and their performances are analysed.\n",
        "\n",
        "**2. Context:** The TPU architecture comprises of a myriad of MAC Units unlike the GPUs or the CPUs which have a lot of advanced data structures like Branch Prediction Units, Hazard Detection and Prevention Unit, Multi-threaded cores, Multiple cores etc to handle the huge computation demands and to maximize the utilization of resources. Unlike the others, TPUs are not for general purposes but instead they are dedicated only for data intensive computations which comprises of a large number of convolutions only for image and data processing applications. The CPUs and GPUs are costly ways to approach the problem whereas the TPUs are cheaper, efficient solutions as they only do convolution operations and no advanced computations are required for inference calculations. The TPUs cannot be used for other applications which involves a lot of predictions, and involving a wide range of mathematical operations etc. The TPUs have a specialized mechanism called quantization to convert the floating point numbers into narrow integers for calculating the results easily.\n",
        "\n",
        "**3. Contributions of the paper to the industry:**\n",
        "\n",
        "* The paper gives us an idea about the interaction between the CPU Host and the TPU. For example, sending the instructions from the CPU to the TPU directly instead of the TPU having to query the CPU everytime for an instruction, which delays the deployment; The fetching of data needed for the computations from the memory into the unified Buffer, etc.\n",
        "\n",
        "* The paper also describes about the different types of instructions involved for neural network based computations. These instructions are CISC type which has an IPC of 10 to 20 clocks per cycle.\n",
        "\n",
        "* The paper discusses the effectiveness of implementing a systolic execution unit which helps in saving energy while keeping all of the matrix multiplication units utilized for the entire duration. The data and control are pipelined whereas the weights fall into the systolic array in a way to maximize the reusability to avoid repeated fetching of the weights from the memory which will add to extra delay.  \n",
        "\n",
        "* A comparision of the CPU, GPU and TPU architecture with a common benchmark model helps us to evaluate the usefulness and efficiency estimates of the TPU on neural network applications. The comparisions are based on the performance of the 3 architectures, the operational intensity of them and how memory bandwidth and computaional units can serve as a barrier in achieving the benchmark is also discussed. These potential problems help us to visualize the problem and apply solutions to it without having to debug the processor from a hardware point of view.\n",
        "\n",
        "* It gives a deep insight into how inspite of being minimal in hardware, TPUs are able to achieve faster response times by exploiting data parallelism and other schemes to fully utilize the resources.\n",
        "\n",
        "* The paper also discusses about the cooling requirements for the TPUs which gives us an insight into the limits of maximizing the utilization of resources while still surpassing the limits of GPUs and CPUs. They introduce us to a new measure called energy-proportionality which describes about how much we utilize the resources and for how much time does this persists.\n",
        "\n",
        "* A comparision of different types of TPU architectures on the basis of geometric mean of the performances, gives us leads into the potential areas of advancements like:\n",
        "\n",
        "        What happens if we increase the memory bandwidth?\n",
        "\n",
        "        What is the performance improvement if Unified Buffer space is increased?\n",
        "\n",
        "        How increasing clock rate can affect the performance, and so on...\n",
        "\n",
        "\n",
        "These aspects of the paper helps us to discover the potential overheads in a TPU architecture and think about the potential possibilities to overcome them to achieve improved performance."
      ],
      "metadata": {
        "id": "0j5A4_Zfbpra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Q2.2 Comprehension (10 Points):\n",
        "- What is a Tensor Processing Unit (TPU) and how does it differ from traditional CPUs and GPUs in terms of design and purpose?\n",
        "- Why is there a need for specialized hardware like TPUs in modern data centers?"
      ],
      "metadata": {
        "id": "IE2n5UQHbuZ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##A2.2:\n",
        "\n",
        "A Tensor Processing Unit is a customized ASIC which is deployed in daracenters to accelerate the inference phase of neural networks.\n",
        "\n",
        "GPUs and CPUs are general purpose processors designed for a wide range of tasks and not for dedicated purposes. GPUs exploit the parallelism in the instruction for improved performance and throughput for huge data-driven applications like handling convolutions, image processing, etc. CPUs are on the other hand used for small data-driven applications like an ordinary addition or multiplication computation purposes.\n",
        "\n",
        "**Differences between TPU and (CPU and GPU)**\n",
        "\n",
        "The major difference lies in the implementation of the hardware architecture among the 3 different types of processors.\n",
        "\n",
        "TPUs do not have extensive hardware units like branch predictors, out-of-order executing pipeline, multi-threaded core, multiple processors, pre-fetch buffers etc., whereas CPUs and GPUs have specialized hardware implemented onto their processor for efficient handling of a wide range of tasks.\n",
        "\n",
        "TPUs are more specifically involved only for tasks involving a large number of matrix multiplications and tensor operations thereby speeding up the response time for NNs. CPUs and GPUs are not domain specific instead for generalized use cases.\n",
        "\n",
        "**Why are specialized hardwares like TPUs needed:**\n",
        "\n",
        "1. Energy efficiency:\n",
        "\n",
        "TPUs are specifically designed to meet the growing demands for handling convolutional and deep neural networks which involves a lot of matrix multiplications. Instead of running the applications on a more generalized hardware like GPUs and CPUs, running the models on TPUs provide increased energy efficiency as all the processing units are populated and utilised efficiently for the matrix multiplication.\n",
        "\n",
        "The units like branch predictors, out-of-order pipelines etc do not find good use in this domain and if we are to use GPUs and TPUs we are underutilizing them and boosting the energy consumption due to those units being turned on almost all of the time. But the lack of these units in TPUs will lead to optimized power consumption and hence save energy ultimately.\n",
        "\n",
        "2. Cost efficient:\n",
        "\n",
        "For specific workloads involving neural networks, the use of GPUs and CPUs can lead to increased cost expenditure as we have advanced data structures but without any use for them. Whereas TPUs have only domain specific hardwares implemented in it and hence is an effective solution for the cost.\n",
        "\n",
        "3. Meeting real-time demands:\n",
        "\n",
        "People expect the response from any computer system to deliver without much response time by reducing the time taken for inferences for which the TPUs serves as a better purpose as they help to reduce the latency by minimizing the computations involved for the inference. They convert the large floating point numbers into 8 bit numbers and thereby reducing the time taken for inferring the output. But the accuracy is ensured by training the models with huge data sets without any quantization to ensure high level of accuracy."
      ],
      "metadata": {
        "id": "9O7k2DbXcNp0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Q2.3 Technical Deep Dive (15 Points):\n",
        "- Describe the architecture of the TPU. How is it optimized for machine learning workloads?\n",
        "- How does the memory hierarchy of the TPU differ from traditional processors, and why is this significant for tensor computations?\n",
        "- What are systolic arrays, and why are they used in TPUs?"
      ],
      "metadata": {
        "id": "PnzcxvhWckY5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A2.3:\n",
        "1. The TPU architecture in general comprises:\n",
        "\n",
        "        A Matrix Multiply unit - for performing the computations required for convolution operations\n",
        "\n",
        "        An Unified Buffer - for temporary partial sum storage purposes\n",
        "\n",
        "        A PCIe interface to communicate with the Host CPU\n",
        "\n",
        "        A DDR DRAM interface to speed up the fetching process from the memory into the TPU.\n",
        "\n",
        "        The control logic for issuing commands to the TPU units etc.\n",
        "\n",
        "        FIFOs to mediate the difference in the production and consumption of data from and to the memory and the CPU Host.\n",
        "\n",
        "  **For machine learning applications** the matrix multiplication unit in the TPU architecture is implemented using systolic arrays which helps the weights to be reused over a period of time when the input data and the control signals are sent to the multiply unit one by one, in a pipelined fashion to enable maximum utilization of resources.\n",
        "\n",
        "  The conversion of the floating point arithmetic into a 8-bit integer arithmetic for fast inference generation is one important characterisitc of the TPUs.\n",
        "\n",
        "  Since the CNN architectures doesn't require much Memory Bandwidth, CNN architectures are mostly exploited in Machine Learning Applications in TPUs.\n",
        "\n",
        "  For Machine learning applications, data parallelism is exploited to distribute the data across the different matrix multiply units by pre-processing the data.\n",
        "\n",
        "2. Memory Heirarchy in TPUs: Usually the memory heirarchy in CPUs comprises of Register file, 3 levels of caches, a DRAM and then the Disk Memory. Here for the TPUs the heirarchy is comprised of Unified Buffer - which is similar to the cache with only one level; then comes the DRAM directly without having any other levels of caches. This helps in reduced access times for the input data and weight access. If we have multiple levels of cache and memory subsystems, they will increase the data access times due to the cache hits and misses etc. Whereas for the TPU architecture, we only have one level of heirarchy which when used with advanced DDR and PCIe interfaces have reduced overhead when looking up for the data. They also have higher bandwidth support to overcome the memory barrier.\n",
        "\n",
        "3. Systolic arrays are matrix multiplication units they helps to exploi the parallelism in data inputs and process them in a pipelined fashion.\n",
        "\n",
        "  The Systolic arrays are used in TPUs to save the energy by reducing the reads and writes of the Unified Buffer, by implementing parallelism among the data elements. This further helps to improve the performance and the throughput to nearly 15 times the original value."
      ],
      "metadata": {
        "id": "sv3rFNjpcuID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Q2.4 Evaluation (25 Points):\n",
        "- How does the TPU's performance compare to contemporary CPUs and GPUs for machine learning tasks?\n",
        "- What benchmarks or workloads were used to evaluate the TPU's performance in the datacenter?\n",
        "- Discuss the TPU's performance per watt. Why is energy efficiency crucial in datacenter environments?\n",
        "- What types of machine learning models and tasks are best suited for TPUs?\n",
        "- How have TPUs impacted the training times of large-scale models at Google?"
      ],
      "metadata": {
        "id": "HRBvFutTc2uh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A2.4:\n",
        "\n",
        "**TPU's performance evaluation in comparision with the CPUs and GPUs:**\n",
        "\n",
        "\n",
        "**Benchmarks used for evaluating the performance of TPUs:**\n",
        "\n",
        "\n",
        "**TPU's performance per Watt:**\n",
        "\n",
        "\n",
        "**Energy is crucial in datacenter environments - Justification:**\n",
        "\n",
        "\n",
        "**Best suited machine learning models for TPUs:**\n",
        "\n",
        "\n",
        "**Training times of large-scale models impacted by TPUs at Google:**"
      ],
      "metadata": {
        "id": "xKUL8nhTdNLZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Q2.5 Contextual Understanding (10 Points):\n",
        "- What are some of the challenges faced in deploying TPUs in datacenters?\n",
        "- Are there specific machine learning tasks or models that TPUs might struggle with compared to other hardware?\n",
        "\n"
      ],
      "metadata": {
        "id": "Fp_FV2bzdV4D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A2.5:\n",
        "**Challenges faced in deploying the TPUs in datacenters:**\n",
        "\n",
        "1. Integrating the TPUs onto the datacenters can be complex as it may require changes to the software frameworks, CPU Deployment Pipelines, Memory Interfaces and overall system architecture.\n",
        "\n",
        "2. The TPU requires knowledge about the underlying hardware architecture to debug in case an issue arises. This requires qualified professionals which could be involving huge costs associated with it.\n",
        "\n",
        "3. The TPU architecture can not be changed depending upon the different data streams. For eg. An CNN based TPU architecture has a different hardware architecture from a LSTM based architecture and hence a good amount of domain knowledge and application knowledge is needed before implementation.\n",
        "\n",
        "4. The workload for a TPU should not be diverse instead be only related to convolution and other multiplication-addition based tasks for the TPU to exploit its capabilities. If a different computation involving data is presented to the TPU the expected performance may not be achieved.\n",
        "\n",
        "5. Since the energy-proportionality is not proportional, the power requirements for the TPUs are huge almost all the time and hence the implementation of cooling subsystems and maintaining power demands almost all the times can be challenging.\n",
        "\n",
        "6. The average performance of a TPU architecture degrades if the current system is scaled to a bigger version. For eg. The current 256x256 matrix multiply unit performance slightly degrades if we go for a 512x512 matrix multiplication unit for LSTM based TPU architectures due to the longer time taken to execute each step though the number of steps get reduced now. And beacuse of this the loading overhead does not cover for the computation overhead.\n",
        "\n",
        "7. The TPU software stack needs to be compatible with the CPUs and GPUs for the applications to be ported quickly to the TPU. So there is a need for the applications to be portable in order for the TPU to be efficiently deployed in the datacenters.\n",
        "\n",
        "\n",
        "**Challenging tasks for TPUs compared to other hardwares:**\n",
        "\n",
        "1. TPUs struggle to deal to sparse data and do not yield the expected performance. As TPUs are designed for being exercised its potential almost all of the running time, if we run a less computationally intensive data on a CPU, then it will not perform as expected.\n",
        "\n",
        "2. If TPUs are used in place of GPUs and CPUs for general purpose tasks like running an ordinary program (for eg. a web browser application) then, it will not perform well as it does not have the dedicated hardware resources for that specific application like the lack of branch predictors, Out-of-Order executing pipelines, multiple threads in a core, multiple cores etc. Whereas a GPU or a CPU can perform atleast half efficient as a TPU for neural network based tasks at the cost of increased hardware architecture for the additional resources.\n",
        "\n",
        "3. If a program having a lot of dependencies in its instruction stream, TPUs fail to exploit the parallelism within it and thereby performs poorly. Whereas CPUs and GPUs are good at it."
      ],
      "metadata": {
        "id": "OvpxYhvNdg18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Q2.6 Discussion and Critique (10 points):\n",
        "- What are the strengths and weaknesses of the paper's methodology and analysis?\n",
        "- Are there any potential biases or assumptions in the paper that you disagree with or find questionable?\n"
      ],
      "metadata": {
        "id": "cxUpnGmtdjfX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A2.6:\n",
        "\n",
        "**Strengths observed:**\n",
        "\n",
        "\n",
        "**Weakness observed:**\n",
        "\n",
        "\n",
        "**Assumptions observed in the paper:**\n"
      ],
      "metadata": {
        "id": "cjAQy1qLdvZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Q2.7 Reflection (10 Points):\n",
        "- How do TPUs fit into the larger trend of custom hardware accelerators for machine learning, such as FPGAs and ASICs?\n",
        "\n",
        "- Discuss the trade-offs between general-purpose hardware (like CPUs) and specialized hardware (like TPUs) in the context of evolving machine learning workloads.\n"
      ],
      "metadata": {
        "id": "hSMCnPg8d_0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A2.7:\n",
        "\n",
        "**TPUs - A fit into the larger trend of customized hardware accelerators:**\n",
        "TPUs are a part of the customized hardware in the sense that they are designed only for handling the convolution based huge computational demands for neural networks. ASICs or FPGAs also serve the very same purpose of serving an SoC implemented for motor control application or digital signal processing application etc.\n",
        "\n",
        "The user-end application is very narrow for all the 3 architectures and are designed for specialized needs instead of serving general purposes. For eg. They are designed only to handle image processing applications and not for radio communication systems or any other system in the same chip.\n",
        "\n",
        "TPUs are implemented to handle the neural network based applications only and not any other applications which involve multiple different types of computations\n",
        "\n",
        "\n",
        "**Trade-offs between CPUs and TPUs:**\n",
        "The TPUs are high-performing hardware accelerators, which gives a boost in the overall performance measured in terms of TOPS/Watt at about 200 times greater than the CPUs.\n",
        "\n",
        "Inference generation is much faster in TPUs than other general purpose architectures due to the approximation of floating point arithmetic to narrow integer arithmetic.\n",
        "\n",
        "TPUs have reduced chip size and low power chip compared to CPUs but CPUs have advanced general purpose architectures which adds for extra space requirement and power requirement.\n",
        "\n",
        "The cost-performance is improved 10x in the TPUs compared with CPUs and others.\n",
        "\n",
        "TPUs do not involve in fetching from the host CPU instead it is given instructions from the CPU and scheduled to execute one by one using the FIFOs. The CPUs directly fetch from the main memory, which also is a reason for increased delay times.\n",
        "\n",
        "The CPUs have CPIs more close to unity typically whereas a TPU have CPIs in the range of 10 - 20.\n",
        "\n",
        "The TPUs have improved power efficiency and are always working most of the time unlike the CPUs, which does not work all the time.\n",
        "\n",
        "In terms of energy proportionality, CPUs have a linear relationship between power, consuming 66% of power at 10% load and the amount of work consumed whereas TPUs do not have a linear relationship as it consumes 88% of power even for 10% of loads.\n"
      ],
      "metadata": {
        "id": "nBtgEgPXeNR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Turn in your reading assignment by saving this answer sheet back to the Github repository."
      ],
      "metadata": {
        "id": "mnRyd9Iie6Dz"
      }
    }
  ]
}