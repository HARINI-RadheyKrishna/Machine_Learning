{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HARINI-RadheyKrishna/Machine_Learning/blob/master/ml-systems-hw5-HARINI-RadheyKrishna-main/EE599_ML_Systems_HW5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NQzcPG3Y7Ar"
      },
      "source": [
        "# HW5 - EE599 Systems for Machine Learning, Fall 2023\n",
        "University of Southern California\n",
        "\n",
        "Instructors: Arash Saifhashemi, Murali Annavaram\n",
        "\n",
        "In this homework assignment, we will ask you to use MPI to implement various types of distributed training paradigms, and then analyze the complexity of each paradigm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OBOUA49Y7As"
      },
      "source": [
        "## Prerequisites:\n",
        "Set the runtime type to GPU. (Runtime -> Change Runtime Type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq8WQREtY7As"
      },
      "source": [
        "## Prepare your Google Drive\n",
        "- Download `ML_Systems_HW5` zip file from GitHub, unzip it, and rename it to `HW5`.\n",
        "- Upload the folder to ``My Drive`` in Google Drive under `ML_Systems` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "52vSfrLqY7At"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/ML_Systems/HW5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCKcyZjYY7At"
      },
      "source": [
        "## Verify that you are in the correct working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KHueXvkxY7At"
      },
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVJLe9ZDY7Au"
      },
      "source": [
        "## Q1\n",
        "Centralized sgd training. Study the code and understand the training loop. Report the final accuracy on test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll2Bry8QY7Au"
      },
      "source": [
        "**Reminder**: set the runtime type to \"GPU\", or your code will run much more slowly on a CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPty-yXCY7Au"
      },
      "outputs": [],
      "source": [
        "!python cent_sgd.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwLmHnM8Y7Au"
      },
      "source": [
        "## Install mpi4py package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qafGT3ZY7Au"
      },
      "source": [
        "MPI, or Message Passing Interface, is a standardized and portable message-passing system designed to enable processes to communicate in a parallel computing environment. MPI has become the de facto standard for high-performance parallel computing in a wide range of applications, from simulations in scientific research to large-scale data processing. At its core, MPI provides various communication mechanisms, including point-to-point and collective operations, allowing data to be exchanged between processes irrespective of their physical locationâ€”be it on the same machine or across a vast cluster of computers. By abstracting the complexities of inter-process communication, MPI empowers developers to craft scalable parallel software efficiently and effectively. Its rich set of functionalities, combined with its performance capabilities, ensures that MPI remains pivotal in the world of parallel and distributed computing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JztuMFIPY7Av"
      },
      "source": [
        "`mpirun` is a command-line utility that comes with most MPI (Message Passing Interface) implementations, facilitating the initiation of parallel jobs across distributed computing environments. Acting as the principal execution tool for MPI programs, `mpirun` launches a specified number of processes across different nodes, allowing these processes to collaborate and communicate as they execute a given MPI-enabled application. The number of processes and their distribution can be controlled by various command-line options and arguments provided to `mpirun`. For instance, using `-n 4` would initiate four parallel processes. Whether running on a local workstation with multiple cores or a large-scale supercomputer, `mpirun` provides developers and researchers a seamless way to scale and manage parallel computations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lmjknpt8Y7Av"
      },
      "outputs": [],
      "source": [
        "!pip install mpi4py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zgck2c2mY7Av"
      },
      "source": [
        "## Q2\n",
        "Using MPI to simulate data parallel distributed training without parameter server. Each rank need to synchronize gradients by all_reduce. Finish the `TODO` lists in the code. Report the final accuracy on test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZzo8fCcY7Av"
      },
      "source": [
        "For colab environment, you will need to append the following arguments to your `mpirun` command: `--allow-run-as-root --oversubscribe`  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yy9wnBMQY7Aw"
      },
      "outputs": [],
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 4 python dist_sgd_serverless.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxBOkBXYY7Aw"
      },
      "source": [
        "## Q3\n",
        "Using MPI to simulate data parallel distributed training with parameter server. Each rank need to send gradients to the server. The server will receive and avergae the gradients. Then, it will update the global model and send it back to each rank. Finish the `TODO` lists in the code. Report the final accuracy on test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4nEc3lAY7Aw"
      },
      "outputs": [],
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 5 python dist_sgd_param_server.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZE95rcsY7Aw"
      },
      "source": [
        "## Q4\n",
        "Using MPI to simulate federated learning with fedavg algorithm. Finish the `TODO` lists in the code. Report the final accuracy on test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GRcIrmEHY7Aw"
      },
      "outputs": [],
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 5 python fed_avg.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4smx7Lz3Y7Ax"
      },
      "source": [
        "## Q5\n",
        "Using MPI to simulate federated learning with fedavg algorithm, but each client has non-IID training data samples. Change `split_method` to `non-iid` and run code again. Check the data distribution plot under `figures` directory and compare it with the `iid` setting. Report the final accuracy on test set. Explain why `non-iid` data distribution may lead to accuracy drop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PxnDr-2uY7Ax"
      },
      "outputs": [],
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 5 python fed_avg.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xtBexcgY7Ax"
      },
      "source": [
        "## Q6\n",
        "Assume the model has `P` trainable parameters and there are `N` processes. For distributed training, we train `S` steps (a step means one update of the model's parameters using a batch of training data). For federated learning, we train `R` rounds. Analyze the total amount of data transmission for each paradigm and quantify it in terms of `P`, `N`, `S`, or `R`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtO2VtmNY7Ax"
      },
      "source": [
        "## Q7\n",
        "What is the time complexity of tree-based reduction? What is the time complexity of ring-based reduction? Assume we have `N` processes, and each communication between two processes takes 1 unit of time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VzNEHnEY7Ax"
      },
      "source": [
        "## Upload files to GitHub\n",
        "Make sure upload your final python code and this IPython notebook to GitHub Repo either mannully or through git commands."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}