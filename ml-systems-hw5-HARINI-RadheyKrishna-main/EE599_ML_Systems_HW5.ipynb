{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HARINI-RadheyKrishna/Machine_Learning/blob/master/ml-systems-hw5-HARINI-RadheyKrishna-main/EE599_ML_Systems_HW5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NQzcPG3Y7Ar"
      },
      "source": [
        "# HW5 - EE599 Systems for Machine Learning, Fall 2023\n",
        "University of Southern California\n",
        "\n",
        "Instructors: Arash Saifhashemi, Murali Annavaram\n",
        "\n",
        "In this homework assignment, we will ask you to use MPI to implement various types of distributed training paradigms, and then analyze the complexity of each paradigm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OBOUA49Y7As"
      },
      "source": [
        "## Prerequisites:\n",
        "Set the runtime type to GPU. (Runtime -> Change Runtime Type)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sq8WQREtY7As"
      },
      "source": [
        "## Prepare your Google Drive\n",
        "- Download `ML_Systems_HW5` zip file from GitHub, unzip it, and rename it to `HW5`.\n",
        "- Upload the folder to ``My Drive`` in Google Drive under `ML_Systems` folder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52vSfrLqY7At",
        "outputId": "ea7b025f-40f7-4996-ec76-454d4589b682"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/ML_Systems/HW5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCKcyZjYY7At"
      },
      "source": [
        "## Verify that you are in the correct working directory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHueXvkxY7At",
        "outputId": "438c8c11-8a68-4efd-f448-b44e02959546"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/ML_Systems/HW5\n"
          ]
        }
      ],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVJLe9ZDY7Au"
      },
      "source": [
        "## Q1\n",
        "Centralized sgd training. Study the code and understand the training loop. Report the final accuracy on test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ll2Bry8QY7Au"
      },
      "source": [
        "**Reminder**: set the runtime type to \"GPU\", or your code will run much more slowly on a CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SPty-yXCY7Au",
        "outputId": "2993f5df-f13c-40e2-8e8d-f2db523e3d74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:Using device cpu\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "INFO:root:Epoch: 0 Batch: 200/500 | Loss: 2.2051\n",
            "INFO:root:Epoch: 0 Batch: 400/500 | Loss: 1.7748\n",
            "INFO:root:Epoch: 1 Batch: 200/500 | Loss: 1.5129\n",
            "INFO:root:Epoch: 1 Batch: 400/500 | Loss: 1.4385\n",
            "INFO:root:Epoch: 2 Batch: 200/500 | Loss: 1.3208\n",
            "INFO:root:Epoch: 2 Batch: 400/500 | Loss: 1.2902\n",
            "INFO:root:Test Accuracy: 0.5550\n"
          ]
        }
      ],
      "source": [
        "!python cent_sgd.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwLmHnM8Y7Au"
      },
      "source": [
        "## Install mpi4py package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qafGT3ZY7Au"
      },
      "source": [
        "MPI, or Message Passing Interface, is a standardized and portable message-passing system designed to enable processes to communicate in a parallel computing environment. MPI has become the de facto standard for high-performance parallel computing in a wide range of applications, from simulations in scientific research to large-scale data processing. At its core, MPI provides various communication mechanisms, including point-to-point and collective operations, allowing data to be exchanged between processes irrespective of their physical location—be it on the same machine or across a vast cluster of computers. By abstracting the complexities of inter-process communication, MPI empowers developers to craft scalable parallel software efficiently and effectively. Its rich set of functionalities, combined with its performance capabilities, ensures that MPI remains pivotal in the world of parallel and distributed computing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JztuMFIPY7Av"
      },
      "source": [
        "`mpirun` is a command-line utility that comes with most MPI (Message Passing Interface) implementations, facilitating the initiation of parallel jobs across distributed computing environments. Acting as the principal execution tool for MPI programs, `mpirun` launches a specified number of processes across different nodes, allowing these processes to collaborate and communicate as they execute a given MPI-enabled application. The number of processes and their distribution can be controlled by various command-line options and arguments provided to `mpirun`. For instance, using `-n 4` would initiate four parallel processes. Whether running on a local workstation with multiple cores or a large-scale supercomputer, `mpirun` provides developers and researchers a seamless way to scale and manage parallel computations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lmjknpt8Y7Av",
        "outputId": "4e4151b4-0435-4026-b753-fdf57f1e22ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mpi4py\n",
            "  Downloading mpi4py-3.1.5.tar.gz (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.1.5-cp310-cp310-linux_x86_64.whl size=2746526 sha256=d2954223b2710b191165c6086abcd5f5985f5ca1bd9f9beeefbe1684d5df597b\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/2b/7f/c852523089e9182b45fca50ff56f49a51eeb6284fd25a66713\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-3.1.5\n"
          ]
        }
      ],
      "source": [
        "!pip install mpi4py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zgck2c2mY7Av"
      },
      "source": [
        "## Q2\n",
        "Using MPI to simulate data parallel distributed training without parameter server. Each rank need to synchronize gradients by all_reduce. Finish the `TODO` lists in the code. Report the final accuracy on test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZzo8fCcY7Av"
      },
      "source": [
        "For colab environment, you will need to append the following arguments to your `mpirun` command: `--allow-run-as-root --oversubscribe`  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yy9wnBMQY7Aw",
        "outputId": "1ef33efb-bbd9-4e12-e72b-ef3d14f95644"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:Rank 1 is using device cpu\n",
            "INFO:root:Rank 2 is using device cpu\n",
            "INFO:root:Rank 0 is using device cpu\n",
            "INFO:root:Rank 3 is using device cpu\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "INFO:root:Epoch: 0 Batch: 200/500 | Loss: 2.2051\n",
            "INFO:root:Epoch: 0 Batch: 400/500 | Loss: 1.7749\n",
            "INFO:root:Epoch: 1 Batch: 200/500 | Loss: 1.5160\n",
            "INFO:root:Epoch: 1 Batch: 400/500 | Loss: 1.4419\n",
            "INFO:root:Epoch: 2 Batch: 200/500 | Loss: 1.3214\n",
            "INFO:root:Epoch: 2 Batch: 400/500 | Loss: 1.2941\n",
            "INFO:root:Test Accuracy: 0.5596\n"
          ]
        }
      ],
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 4 python dist_sgd_serverless.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxBOkBXYY7Aw"
      },
      "source": [
        "## Q3\n",
        "Using MPI to simulate data parallel distributed training with parameter server. Each rank need to send gradients to the server. The server will receive and avergae the gradients. Then, it will update the global model and send it back to each rank. Finish the `TODO` lists in the code. Report the final accuracy on test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "P4nEc3lAY7Aw",
        "outputId": "2b4fc4a9-99c9-4d14-ab5b-8f1bf9c76e66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:Rank 1 is using device cpu\n",
            "INFO:root:Rank 3 is using device cpu\n",
            "INFO:root:Rank 0 is using device cpu\n",
            "INFO:root:Rank 2 is using device cpu\n",
            "INFO:root:Rank 4 is using device cpu\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "INFO:root:Epoch: 0 Batch: 200/500 | Loss: 2.2052\n",
            "INFO:root:Epoch: 0 Batch: 400/500 | Loss: 1.7747\n",
            "INFO:root:Epoch: 1 Batch: 200/500 | Loss: 1.5175\n",
            "INFO:root:Epoch: 1 Batch: 400/500 | Loss: 1.4420\n",
            "INFO:root:Epoch: 2 Batch: 200/500 | Loss: 1.3228\n",
            "INFO:root:Epoch: 2 Batch: 400/500 | Loss: 1.2930\n",
            "INFO:root:Test Accuracy: 0.5503\n"
          ]
        }
      ],
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 5 python dist_sgd_param_server.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZE95rcsY7Aw"
      },
      "source": [
        "## Q4\n",
        "Using MPI to simulate federated learning with fedavg algorithm. Finish the `TODO` lists in the code. Report the final accuracy on test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GRcIrmEHY7Aw",
        "outputId": "9d1b94ba-ea83-440c-ee8d-ee30c0698ae2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:Rank 2 is using device cpu\n",
            "INFO:root:Rank 3 is using device cpu\n",
            "INFO:root:Rank 0 is using device cpu\n",
            "INFO:root:Rank 4 is using device cpu\n",
            "INFO:root:Rank 1 is using device cpu\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "INFO:root:Client: 3 | Round: 0 | Loss: 0.5875 | Accuracy: 0.1012\n",
            "INFO:root:Client: 1 | Round: 0 | Loss: 1.0303 | Accuracy: 0.2036\n",
            "INFO:root:Client: 2 | Round: 0 | Loss: 0.9861 | Accuracy: 0.2488\n",
            "INFO:root:Client: 4 | Round: 0 | Loss: 0.8736 | Accuracy: 0.2452\n",
            "INFO:root:--------- | Round: 0 | aggregation finished | ---------\n",
            "INFO:root:Client: 3 | Round: 1 | Loss: 0.4840 | Accuracy: 0.1044\n",
            "INFO:root:Client: 1 | Round: 1 | Loss: 0.8430 | Accuracy: 0.2076\n",
            "INFO:root:Client: 4 | Round: 1 | Loss: 0.6242 | Accuracy: 0.2444\n",
            "INFO:root:Client: 2 | Round: 1 | Loss: 0.7717 | Accuracy: 0.2484\n",
            "INFO:root:--------- | Round: 1 | aggregation finished | ---------\n",
            "INFO:root:Client: 3 | Round: 2 | Loss: 0.4527 | Accuracy: 0.1708\n",
            "INFO:root:Client: 1 | Round: 2 | Loss: 0.7762 | Accuracy: 0.2216\n",
            "INFO:root:Client: 4 | Round: 2 | Loss: 0.5334 | Accuracy: 0.2516\n",
            "INFO:root:Client: 2 | Round: 2 | Loss: 0.6982 | Accuracy: 0.2612\n",
            "INFO:root:--------- | Round: 2 | aggregation finished | ---------\n",
            "INFO:root:Client: 3 | Round: 3 | Loss: 0.3973 | Accuracy: 0.2176\n",
            "INFO:root:Client: 1 | Round: 3 | Loss: 0.7547 | Accuracy: 0.2160\n",
            "INFO:root:Client: 4 | Round: 3 | Loss: 0.4943 | Accuracy: 0.2656\n",
            "INFO:root:Client: 2 | Round: 3 | Loss: 0.6504 | Accuracy: 0.2956\n",
            "INFO:root:--------- | Round: 3 | aggregation finished | ---------\n",
            "INFO:root:Client: 3 | Round: 4 | Loss: 0.3731 | Accuracy: 0.1608\n",
            "INFO:root:Client: 1 | Round: 4 | Loss: 0.7179 | Accuracy: 0.2084\n",
            "INFO:root:Client: 2 | Round: 4 | Loss: 0.6181 | Accuracy: 0.2920\n",
            "INFO:root:Client: 4 | Round: 4 | Loss: 0.4608 | Accuracy: 0.2520\n",
            "INFO:root:--------- | Round: 4 | aggregation finished | ---------\n"
          ]
        }
      ],
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 5 python fed_avg.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4smx7Lz3Y7Ax"
      },
      "source": [
        "## Q5\n",
        "Using MPI to simulate federated learning with fedavg algorithm, but each client has non-IID training data samples. Change `split_method` to `non-iid` and run code again. Check the data distribution plot under `figures` directory and compare it with the `iid` setting. Report the final accuracy on test set. Explain why `non-iid` data distribution may lead to accuracy drop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "PxnDr-2uY7Ax",
        "outputId": "c2a8ef9e-902e-4126-93c0-5aa169d9ec0d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:root:Rank 2 is using device cpu\n",
            "INFO:root:Rank 4 is using device cpu\n",
            "INFO:root:Rank 0 is using device cpu\n",
            "INFO:root:Rank 1 is using device cpu\n",
            "INFO:root:Rank 3 is using device cpu\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "INFO:root:Client: 3 | Round: 0 | Loss: 0.5875 | Accuracy: 0.1012\n",
            "INFO:root:Client: 1 | Round: 0 | Loss: 1.0303 | Accuracy: 0.2036\n",
            "INFO:root:Client: 2 | Round: 0 | Loss: 0.9861 | Accuracy: 0.2488\n",
            "INFO:root:Client: 4 | Round: 0 | Loss: 0.8736 | Accuracy: 0.2452\n",
            "INFO:root:--------- | Round: 0 | aggregation finished | ---------\n",
            "INFO:root:Client: 3 | Round: 1 | Loss: 0.4840 | Accuracy: 0.1044\n",
            "INFO:root:Client: 1 | Round: 1 | Loss: 0.8430 | Accuracy: 0.2076\n",
            "INFO:root:Client: 4 | Round: 1 | Loss: 0.6242 | Accuracy: 0.2444\n",
            "INFO:root:Client: 2 | Round: 1 | Loss: 0.7717 | Accuracy: 0.2484\n",
            "INFO:root:--------- | Round: 1 | aggregation finished | ---------\n",
            "INFO:root:Client: 3 | Round: 2 | Loss: 0.4527 | Accuracy: 0.1708\n",
            "INFO:root:Client: 2 | Round: 2 | Loss: 0.6982 | Accuracy: 0.2612\n",
            "INFO:root:Client: 1 | Round: 2 | Loss: 0.7762 | Accuracy: 0.2216\n",
            "INFO:root:Client: 4 | Round: 2 | Loss: 0.5334 | Accuracy: 0.2516\n",
            "INFO:root:--------- | Round: 2 | aggregation finished | ---------\n",
            "INFO:root:Client: 3 | Round: 3 | Loss: 0.3973 | Accuracy: 0.2176\n",
            "INFO:root:Client: 1 | Round: 3 | Loss: 0.7547 | Accuracy: 0.2160\n",
            "INFO:root:Client: 2 | Round: 3 | Loss: 0.6504 | Accuracy: 0.2956\n",
            "INFO:root:Client: 4 | Round: 3 | Loss: 0.4943 | Accuracy: 0.2656\n",
            "INFO:root:--------- | Round: 3 | aggregation finished | ---------\n",
            "INFO:root:Client: 3 | Round: 4 | Loss: 0.3731 | Accuracy: 0.1608\n",
            "INFO:root:Client: 1 | Round: 4 | Loss: 0.7179 | Accuracy: 0.2084\n",
            "INFO:root:Client: 4 | Round: 4 | Loss: 0.4608 | Accuracy: 0.2520\n",
            "INFO:root:Client: 2 | Round: 4 | Loss: 0.6181 | Accuracy: 0.2920\n",
            "INFO:root:--------- | Round: 4 | aggregation finished | ---------\n"
          ]
        }
      ],
      "source": [
        "!mpirun --allow-run-as-root --oversubscribe -n 5 python fed_avg_non_iid.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-xtBexcgY7Ax"
      },
      "source": [
        "## Q6\n",
        "Assume the model has `P` trainable parameters and there are `N` processes. For distributed training, we train `S` steps (a step means one update of the model's parameters using a batch of training data). For federated learning, we train `R` rounds. Analyze the total amount of data transmission for each paradigm and quantify it in terms of `P`, `N`, `S`, or `R`."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Distributed learning:\n",
        "Parameter count = N x P for 1 step\n",
        "So, for S steps, Parameter count = N x P x S\n",
        "For 1 transmission consisting of 2 rounds,\n",
        "\n",
        "Amount of data transmission = 2 x N x P x S\n",
        "\n",
        "\n",
        "Federated learning:\n",
        "For R rounds, the amount of data transmission = 2 x N x P x R"
      ],
      "metadata": {
        "id": "uLgaWDp0ulo-"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtO2VtmNY7Ax"
      },
      "source": [
        "## Q7\n",
        "What is the time complexity of tree-based reduction? What is the time complexity of ring-based reduction? Assume we have `N` processes, and each communication between two processes takes 1 unit of time."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Time complexity of tree based reduction = O(log N)\n",
        "\n",
        "Time complexity of Ring based reduction = O(N)"
      ],
      "metadata": {
        "id": "SqV3pL4TvsFP"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8VzNEHnEY7Ax"
      },
      "source": [
        "## Upload files to GitHub\n",
        "Make sure upload your final python code and this IPython notebook to GitHub Repo either mannully or through git commands."
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}